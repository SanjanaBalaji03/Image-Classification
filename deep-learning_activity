from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import os
import matplotlib.pyplot as plt
import tensorflow as tf
import PIL
import pathlib

from PIL import Image
from pathlib import Path
from tensorflow.keras import layers,models

loaded_data=Path("/content/drive/My Drive/DL")

total_image_count=0

dress_categories=list(loaded_data.glob('*'))
if len(dress_categories)>0:
   for dress_categories in dress_categories:
       dress_images=list(dress_categories.glob('*.jpg'))+list(dress_categories('*.png'))
       if len(dress_images)>0:
          total_image_count+=len(dress_images)
   print("Total number of dress images found:",total_image_count)
else:
print("No dress categories (subfolders) found in the directory.)

set_height, set_width=180,180
batch_size=32

training_images = tf.keras.preprocessing.image_dataset_from_directory(
    loaded_data,
    subset="training",
    validation_split = 0.25,
    seed=123,
    image_size=(set_height, set_width),
    batch_size=batch_size
)
validation_images = tf.keras.preprocessing.image_dataset_from_directory(
    loaded_data,
    subset="validation",  # Use "validation" for the validation subset
    validation_split=0.25,  # Adjust this value as needed
    seed=123,
    image_size=(set_height, set_width),
    batch_size=batch_size
)
dress_classes = training_images.class_names
print(dress_classes)
dataset_classes = 3
from tensorflow.keras.models import Sequential
model = Sequential([])
# Hyperparameter tuning setup
dropout_rates = [0.0, 0.2, 0.4, 0.6]  # Define a list of dropout rates to try
best_model = None
best_accuracy = 0.6
dropout_rate = [0.4]

layers.experimental.preprocessing.Rescaling(1./255, input_shape=(set_height, set_width, 3))
model.add(layers.Conv2D(16, 3, padding='same', activation='relu'))
model.add(layers.MaxPooling2D())
model.add(layers.Conv2D(32, 3, padding='same', activation='relu'))
model.add(layers.MaxPooling2D())
model.add(layers.Conv2D(64, 3, padding='same', activation='relu'))
model.add(layers.MaxPooling2D())
model.add(layers.Conv2D(128, 3, padding='same', activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(dataset_classes,activation='softmax'))
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

epochs=25

CNN_model = model.fit(
  training_images,
  validation_data=validation_images,
  epochs=epochs
)

# Evaluate the model on validation data
val_accuracy = CNN_model.history['val_accuracy'][-1]
print(f"Validation accuracy with dropout rate {dropout_rate}: {val_accuracy}")

# Check if this model is the best so far
if val_accuracy > best_accuracy:
   best_accuracy = val_accuracy
   best_model = model

def predict_input_image(img):
  img_4d=img.reshape(-1,180,180,3)
  prediction=model.predict(img_4d)[0]
  return {dress_classes[i]: float(prediction[i]) for i in range(3)}
  # Save the best model to a file
model.save('/content/drive/My Drive/DL/best_model.h5')

!pip install gradio
import gradio as gr
image = gr.inputs.Image(shape=(180,180))

label = gr.outputs.Label(num_top_classes=3)

gr.Interface(fn=predict_input_image, inputs=image, outputs=label,interpretation='default').launch(debug='True',share=True)
